{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005fb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Callable, List, Tuple\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Try to import TrustCall, fallback to structured output if not available\n",
    "try:\n",
    "    from trustcall import create_extractor\n",
    "    TRUSTCALL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRUSTCALL_AVAILABLE = False\n",
    "\n",
    "# Define a utility function to add new items to a list while avoiding duplicates\n",
    "def _add_new(left: list, right: list) -> list:\n",
    "    \"\"\"Extend left list with new items from right list.\"\"\"\n",
    "    return left + [item for item in right if item not in set(left)]\n",
    "\n",
    "\n",
    "# Define the state for the agent, which includes selected tool IDs\n",
    "class State(MessagesState):\n",
    "    selected_tool_ids: Annotated[list[str], _add_new]\n",
    "\n",
    "# Define a structured response model for tool selection\n",
    "class ToolSelectionResponse(BaseModel):\n",
    "    \"\"\"Structured response for tool selection.\"\"\"\n",
    "    tool_ids: List[str] = Field(\n",
    "        description=\"List of tool IDs selected for the task\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Brief explanation of why these tools were selected\"\n",
    "    )\n",
    "\n",
    "# Define a function to create an agent with LLM-driven tool selection\n",
    "def create_agent(\n",
    "    selector_llm: LanguageModelLike,\n",
    "    main_llm: LanguageModelLike,\n",
    "    tool_registry: dict[str, BaseTool | Callable],\n",
    "    tool_selection_limit: int = 10,\n",
    "    prompt: ChatPromptTemplate | None = None,\n",
    ") -> StateGraph:\n",
    "    \"\"\"Create an agent with LLM-driven tool selection.\n",
    "\n",
    "    The agent uses a two-node architecture:\n",
    "    1. Tool Selector: A fast LLM that selects relevant tools from the registry\n",
    "    2. Main Agent: A ReAct agent that uses only the selected tools\n",
    "\n",
    "    Args:\n",
    "        selector_llm: Fast language model for tool selection.\n",
    "        main_llm: Language model for the main agent execution.\n",
    "        tool_registry: Dict mapping string IDs to tools or callables.\n",
    "        tool_selection_limit: The maximum number of tools to select.\n",
    "    \"\"\"\n",
    "    \n",
    "    # === 1. DEFINE THE PROMPT FOR THE MAIN AGENT ===\n",
    "    if prompt is None:\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"You are a powerful and helpful assistant. Your goal is to use a pre-selected set of tools to answer the user's query accurately.\n",
    "\n",
    "The tools available for this task are bound to you directly. You do not need to see a list of them in this prompt.\n",
    "\n",
    "**Operational Principles:**\n",
    "\n",
    "1.  **Analyze the Goal:** Carefully examine the user's latest query and the conversation history to understand the objective.\n",
    "2.  **Use Your Tools:** You have been provided with a specific, curated set of tools that are deemed relevant for this task. Your primary job is to use them effectively.\n",
    "3.  **ReAct Workflow:** Follow a \"Reason-Act\" loop to solve the problem:\n",
    "    - **Thought:** Explain your reasoning. What are you trying to achieve, and which tool will you use to do it?\n",
    "    - **Action:** State the exact tool and input you are using.\n",
    "    - **Observation:** After a tool is used, you will see the result.\n",
    "    - **Thought:** Analyze the result. Do you have the final answer, or do you need to use another tool? Repeat until you can answer the user's query.\n",
    "4.  **Final Answer:** Once you have sufficient information from your tools, provide a clear, concise, and final answer to the user. Do not explain your internal tool-use process in the final answer.\n",
    "\"\"\",\n",
    "                ),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                # MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # === 2. DEFINE THE TOOL SELECTION LOGIC ===\n",
    "    \n",
    "    def _create_tool_manifest(tool_registry: dict[str, BaseTool | Callable]) -> str:\n",
    "        \"\"\"Create a structured manifest of all available tools.\"\"\"\n",
    "        manifest_lines = [\"# Available Tools\\n\"]\n",
    "        for tool_id, tool in tool_registry.items():\n",
    "            name = tool.name if isinstance(tool, BaseTool) else tool.__name__\n",
    "            description = tool.description if isinstance(tool, BaseTool) else tool.__doc__ or \"No description available\"\n",
    "            manifest_lines.extend([f\"## {name}\", f\"- **ID**: {tool_id}\", f\"- **Description**: {description}\", \"\"])\n",
    "        return \"\\n\".join(manifest_lines)\n",
    "\n",
    "    def _get_selection_prompt(user_query: str, tool_manifest: str) -> str:\n",
    "        \"\"\"Generate the system prompt for the tool selector.\"\"\"\n",
    "        return f\"\"\"You are a tool selection expert. Your task is to analyze a user's query and select the most relevant tools from the available tool registry.\n",
    "\n",
    "{tool_manifest}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the user's query carefully.\n",
    "2. Select 1-{tool_selection_limit} of the most relevant tools that could help answer the query.\n",
    "3. Provide the tool IDs (not names) in your response.\n",
    "4. If no tools are relevant to the query, return an empty list.\n",
    "5. Explain your reasoning briefly.\n",
    "\n",
    "User Query: {user_query}\"\"\"\n",
    "\n",
    "    if TRUSTCALL_AVAILABLE:\n",
    "        tool_selector_extractor = create_extractor(selector_llm, tools=[ToolSelectionResponse], tool_choice=\"ToolSelectionResponse\")\n",
    "        \n",
    "        def _invoke_tool_selector(sample_prompt: str) -> ToolSelectionResponse:\n",
    "            result = tool_selector_extractor.invoke({\"messages\": [{\"role\": \"user\", \"content\": f\"Select the most relevant tools for this query:\\n{sample_prompt}\"}]})\n",
    "            return result[\"responses\"][0]\n",
    "    else:\n",
    "        selector_with_structured_output = selector_llm.with_structured_output(ToolSelectionResponse)\n",
    "        def _invoke_tool_selector(sample_prompt: str) -> ToolSelectionResponse:\n",
    "            return selector_with_structured_output.invoke([SystemMessage(content=sample_prompt)])\n",
    "\n",
    "    def tool_selector(state: State, config: RunnableConfig) -> dict:\n",
    "        \"\"\"Selects relevant tools based on the user's query.\"\"\"\n",
    "        user_query = next((msg.content for msg in reversed(state[\"messages\"]) if isinstance(msg, HumanMessage)), \"\")\n",
    "        tool_manifest = _create_tool_manifest(tool_registry)\n",
    "        system_prompt = _get_selection_prompt(user_query, tool_manifest)\n",
    "        tool_selection = _invoke_tool_selector(system_prompt)\n",
    "        return {\n",
    "            \"selected_tool_ids\": tool_selection.tool_ids,\n",
    "            \"messages\": [AIMessage(content=f\"Selected tools: {tool_selection.tool_ids}. Reasoning: {tool_selection.reasoning}\")],\n",
    "        }\n",
    "\n",
    "    # === 3. DEFINE THE MAIN AGENT AND GRAPH STRUCTURE (FIXED) ===\n",
    "    \n",
    "    def main_agent(state: State) -> dict:\n",
    "        \"\"\"This node runs the main agent logic with the selected set of tools.\"\"\"\n",
    "        selected_tools = [\n",
    "            tool_registry[tool_id] for tool_id in state.get(\"selected_tool_ids\", [])\n",
    "            if tool_id in tool_registry\n",
    "        ]\n",
    "        \n",
    "        # Create the agent runnable with the dynamically selected tools.\n",
    "        agent_runnable = prompt | main_llm.bind_tools(selected_tools)\n",
    "        \n",
    "        # Invoke the runnable with only the 'messages' from the state.\n",
    "        # The 'agent_scratchpad' is correctly derived from this by the placeholder.\n",
    "        response = agent_runnable.invoke({\"messages\": state[\"messages\"]})\n",
    "        \n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "    # The ToolNode executes tool calls requested by the main agent.\n",
    "    tool_node = ToolNode(\n",
    "        [tool for tool in tool_registry.values() if isinstance(tool, (BaseTool, Callable))]\n",
    "    )\n",
    "\n",
    "    def should_continue(state: State) -> str:\n",
    "        \"\"\"Determines if the agent should call tools or end.\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        return \"tools\" if hasattr(last_message, \"tool_calls\") and last_message.tool_calls else END\n",
    "\n",
    "    # Build the graph\n",
    "    builder = StateGraph(State)\n",
    "    \n",
    "    builder.add_node(\"tool_selector\", tool_selector)\n",
    "    builder.add_node(\"main_agent\", main_agent)\n",
    "    builder.add_node(\"tools\", tool_node)\n",
    "    \n",
    "    # Define the graph's flow\n",
    "    builder.set_entry_point(\"tool_selector\")\n",
    "    builder.add_edge(\"tool_selector\", \"main_agent\")\n",
    "    \n",
    "    # This conditional edge creates the ReAct loop.\n",
    "    builder.add_conditional_edges(\n",
    "        \"main_agent\",\n",
    "        should_continue,\n",
    "        {\"tools\": \"tools\", END: END},\n",
    "    )\n",
    "    # After tools are executed, the result is sent back to the main_agent.\n",
    "    builder.add_edge(\"tools\", \"main_agent\")\n",
    "    \n",
    "    return builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154bb9aa",
   "metadata": {},
   "source": [
    "> Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ff45eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Agent ---\n",
      "--- Agent Initialized Successfully ---\n",
      "\n",
      "--- Running Demonstration Case 1: SINGLE TOOL USE (FACTORIAL) ---\n",
      "\n",
      "Initial Conversation:\n",
      "  HUMAN: What is the factorial of 6?\n",
      "--------------------------------------------------\n",
      "\n",
      "Final Agent State:\n",
      "  - Selected Tool IDs:\n",
      "    ['b3c37f3b-7454-4fff-96b0-5531910e0115']\n",
      "\n",
      "  - Final Conversation History:\n",
      "    HUMAN: What is the factorial of 6?\n",
      "    AI: Selected tools: ['b3c37f3b-7454-4fff-96b0-5531910e0115']. Reasoning: The query asks for the factorial of 6, which is a mathematical operation that calculates the product of all positive integers up to a given number. The 'factorial' tool is specifically designed to compute the factorial of a number, making it the most relevant tool for this query.\n",
      "    AI: Tool Calls: factorial({'n': 6})\n",
      "    TOOL: 720\n",
      "    AI: The factorial of 6 is 720.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Demonstration Case 2: CONVERSATIONAL HISTORY (SQUARE ROOT) ---\n",
      "\n",
      "Initial Conversation:\n",
      "  HUMAN: My favorite number is 8.\n",
      "  AI: Got it. Your favorite number is 8. How can I help?\n",
      "  HUMAN: What is its square root?\n",
      "--------------------------------------------------\n",
      "\n",
      "Final Agent State:\n",
      "  - Selected Tool IDs:\n",
      "    ['31f578a7-3f22-4313-84c1-fa25f3216e83']\n",
      "\n",
      "  - Final Conversation History:\n",
      "    HUMAN: My favorite number is 8.\n",
      "    AI: Got it. Your favorite number is 8. How can I help?\n",
      "    HUMAN: What is its square root?\n",
      "    AI: Selected tools: ['31f578a7-3f22-4313-84c1-fa25f3216e83']. Reasoning: The query asks for the square root of a number. The most relevant tool for this task is the 'sqrt' tool, which is specifically designed to return the square root of a given input.\n",
      "    AI: Tool Calls: sqrt({'x': 8})\n",
      "    TOOL: 2.8284271247461903\n",
      "    AI: The square root of 8 is approximately 2.83.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Demonstration Case 3: MULTI-TOOL SELECTION (SINE & COSINE) ---\n",
      "\n",
      "Initial Conversation:\n",
      "  HUMAN: What is the sine of 1.57 and the cosine of 0?\n",
      "--------------------------------------------------\n",
      "\n",
      "Final Agent State:\n",
      "  - Selected Tool IDs:\n",
      "    ['44f240c4-3522-45bb-801f-45dc834ffd99', '9569f491-7253-4fd1-8167-41d76f5a740d']\n",
      "\n",
      "  - Final Conversation History:\n",
      "    HUMAN: What is the sine of 1.57 and the cosine of 0?\n",
      "    AI: Selected tools: ['44f240c4-3522-45bb-801f-45dc834ffd99', '9569f491-7253-4fd1-8167-41d76f5a740d']. Reasoning: The query asks for the sine of 1.57 and the cosine of 0. The most relevant tools for this task are the 'sin' tool for calculating the sine of a given angle in radians, and the 'cos' tool for calculating the cosine of a given angle in radians. These tools directly provide the trigonometric functions needed to answer the query.\n",
      "    AI: Tool Calls: sin({'x': 1.57}), cos({'x': 0})\n",
      "    TOOL: 0.9999996829318346\n",
      "    TOOL: 1.0\n",
      "    AI: The sine of 1.57 is approximately 0.9999997, and the cosine of 0 is 1.0.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Demonstration Case 4: TWO-ARGUMENT TOOL (LOG) ---\n",
      "\n",
      "Initial Conversation:\n",
      "  HUMAN: What is the log of 1024 with a base of 2?\n",
      "--------------------------------------------------\n",
      "\n",
      "Final Agent State:\n",
      "  - Selected Tool IDs:\n",
      "    ['d18a74a9-f76e-4135-bcb1-77888bff7cee']\n",
      "\n",
      "  - Final Conversation History:\n",
      "    HUMAN: What is the log of 1024 with a base of 2?\n",
      "    AI: Selected tools: ['d18a74a9-f76e-4135-bcb1-77888bff7cee']. Reasoning: The query asks for the logarithm of 1024 with base 2. The most relevant tool for this task is the 'log2' function, which directly computes the base 2 logarithm of a number. Therefore, the tool with ID 'd18a74a9-f76e-4135-bcb1-77888bff7cee' is selected.\n",
      "    AI: Tool Calls: log2({'x': 1024})\n",
      "    TOOL: 10.0\n",
      "    AI: The logarithm of 1024 with a base of 2 is 10.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Demonstration Case 5: NO TOOL SCENARIO (GENERAL KNOWLEDGE) ---\n",
      "\n",
      "Initial Conversation:\n",
      "  HUMAN: What is the capital of France?\n",
      "--------------------------------------------------\n",
      "\n",
      "Final Agent State:\n",
      "  - Selected Tool IDs:\n",
      "    []\n",
      "\n",
      "  - Final Conversation History:\n",
      "    HUMAN: What is the capital of France?\n",
      "    AI: Selected tools: []. Reasoning: The query asks for the capital of France, which is a geographical question. None of the available mathematical or computational tools are relevant to answering this query, as they are designed for mathematical calculations and operations, not for providing geographical or factual information about countries.\n",
      "    AI: The capital of France is Paris.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Demonstration Case 6: EDGE CASE (CREATIVE REQUEST) ---\n",
      "\n",
      "Initial Conversation:\n",
      "  HUMAN: Tell me a short poem about programming.\n",
      "--------------------------------------------------\n",
      "\n",
      "Final Agent State:\n",
      "  - Selected Tool IDs:\n",
      "    []\n",
      "\n",
      "  - Final Conversation History:\n",
      "    HUMAN: Tell me a short poem about programming.\n",
      "    AI: Selected tools: []. Reasoning: The user's query is asking for a short poem about programming, which is a creative task. The available tools are mathematical and computational functions, which are not relevant to generating or analyzing poetry. Therefore, no tools from the list are applicable to this query.\n",
      "    AI: In lines of code, a world we weave,  \n",
      "With logic's thread, our dreams conceive.  \n",
      "From zeroes and ones, we craft and mold,  \n",
      "A digital tale, in bytes untold.  \n",
      "\n",
      "Through loops and functions, we explore,  \n",
      "A universe vast, forever more.  \n",
      "In every bug, a lesson learned,  \n",
      "In every fix, a triumph earned.  \n",
      "\n",
      "With keys we dance, in rhythmic flow,  \n",
      "Creating worlds, where ideas grow.  \n",
      "In programming's art, we find our way,  \n",
      "Building tomorrow, line by line, today.  \n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import types\n",
    "import uuid\n",
    "from typing import Callable\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from a .env file if present\n",
    "\n",
    "import pytest\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langsmart_bigtool import create_agent\n",
    "# Assuming the new utils file is in the correct path\n",
    "from langsmart_bigtool.utils import convert_positional_only_function_to_tool\n",
    "\n",
    "# --- Tool and Model Setup ---\n",
    "\n",
    "# Set your OpenAI API key here.\n",
    "# It's recommended to use environment variables for security.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    # This is a placeholder and will not work.\n",
    "    # Replace it with your actual OpenAI API key.\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "\n",
    "# Create a list of all the functions in the math module\n",
    "all_names = dir(math)\n",
    "math_functions = [\n",
    "    getattr(math, name)\n",
    "    for name in all_names\n",
    "    if isinstance(getattr(math, name), types.BuiltinFunctionType)\n",
    "]\n",
    "\n",
    "# Convert to tools using the revised utility function\n",
    "all_tools = []\n",
    "for function in math_functions:\n",
    "    if wrapper := convert_positional_only_function_to_tool(function):\n",
    "        all_tools.append(wrapper)\n",
    "\n",
    "# Store tool objects in a registry with unique IDs\n",
    "tool_registry = {str(uuid.uuid4()): tool for tool in all_tools}\n",
    "\n",
    "\n",
    "def _get_tool_id_by_name(tool_name: str) -> str:\n",
    "    \"\"\"Gets the unique ID of a tool by its registered name.\"\"\"\n",
    "    for tool_id, tool in tool_registry.items():\n",
    "        if isinstance(tool, BaseTool) and tool.name == tool_name:\n",
    "            return tool_id\n",
    "    raise ValueError(f\"Tool with name '{tool_name}' not found in the registry.\")\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def llm():\n",
    "    \"\"\"Provides a reusable ChatOpenAI model instance for all tests.\"\"\"\n",
    "    # Using a powerful model is recommended for reliable tool selection\n",
    "    return ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def agent(llm: LanguageModelLike):\n",
    "    \"\"\"Provides a compiled agent instance for all tests.\"\"\"\n",
    "    # The create_agent function returns a compiled graph directly.\n",
    "    builder = create_agent(llm, llm, tool_registry)\n",
    "    return builder\n",
    "\n",
    "\n",
    "# --- Test Cases ---\n",
    "\n",
    "def test_state_management_and_tool_use(agent):\n",
    "    \"\"\"\n",
    "    Tests that the agent correctly selects a tool, executes it, and\n",
    "    preserves the full message history.\n",
    "    \"\"\"\n",
    "    acos_tool_id = _get_tool_id_by_name(\"acos\")\n",
    "    initial_query = \"Calculate the arc cosine of 0.5\"\n",
    "\n",
    "    result = agent.invoke({\"messages\": [HumanMessage(content=initial_query)]})\n",
    "\n",
    "    # 1. Verify that the original user message is preserved\n",
    "    messages = result[\"messages\"]\n",
    "    assert any(\n",
    "        isinstance(msg, HumanMessage) and initial_query in msg.content for msg in messages\n",
    "    ), \"Original user message should be preserved in the final state.\"\n",
    "\n",
    "    # 2. Verify the correct tool was selected\n",
    "    assert acos_tool_id in result[\"selected_tool_ids\"], \"The 'acos' tool should have been selected.\"\n",
    "\n",
    "    # 3. Verify a tool message with the result is present\n",
    "    assert any(\n",
    "        isinstance(msg, ToolMessage) and \"1.047\" in msg.content for msg in messages\n",
    "    ), \"A ToolMessage with the approximate result of acos(0.5) should be present.\"\n",
    "\n",
    "    # 4. Verify the final AI message contains the answer\n",
    "    final_message = messages[-1]\n",
    "    assert isinstance(final_message, AIMessage)\n",
    "    assert \"1.047\" in final_message.content, \"The final AI message should contain the calculated answer.\"\n",
    "\n",
    "\n",
    "def test_reasoning_and_tool_selection_message(agent):\n",
    "    \"\"\"\n",
    "    Tests that the agent's reasoning for tool selection is captured\n",
    "    in an AIMessage.\n",
    "    \"\"\"\n",
    "    query = \"What is the factorial of 6?\"\n",
    "    result = agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # Find the AIMessage that explains the tool selection\n",
    "    reasoning_message = next(\n",
    "        (msg for msg in messages if isinstance(msg, AIMessage) and \"Selected tools:\" in msg.content),\n",
    "        None,\n",
    "    )\n",
    "\n",
    "    assert reasoning_message is not None, \"A reasoning message for tool selection should be present.\"\n",
    "    assert \"factorial\" in reasoning_message.content, \"The reasoning should mention the selected 'factorial' tool.\"\n",
    "\n",
    "\n",
    "def test_main_agent_uses_full_conversation_history(agent):\n",
    "    \"\"\"\n",
    "    Tests that the main agent uses the context from a multi-turn conversation\n",
    "    to resolve ambiguity and execute the correct tool.\n",
    "    \"\"\"\n",
    "    initial_messages = [\n",
    "        HumanMessage(content=\"My favorite number is 8.\"),\n",
    "        AIMessage(content=\"Got it. Your favorite number is 8. How can I help you with it?\"),\n",
    "        HumanMessage(content=\"What is its square root?\"),\n",
    "    ]\n",
    "\n",
    "    result = agent.invoke({\"messages\": initial_messages})\n",
    "    final_message = result[\"messages\"][-1]\n",
    "\n",
    "    # The agent should use the context (\"8\") to calculate the square root\n",
    "    assert isinstance(final_message, AIMessage)\n",
    "    assert \"2.828\" in final_message.content, \"The agent should have calculated the square root of 8 using conversation history.\"\n",
    "\n",
    "\n",
    "def test_no_valid_tools_scenario(agent):\n",
    "    \"\"\"\n",
    "    Tests that the agent responds conversationally without selecting a tool\n",
    "    when the query does not match any available tools.\n",
    "    \"\"\"\n",
    "    result = agent.invoke({\"messages\": [HumanMessage(content=\"What is the capital of France?\")]})\n",
    "\n",
    "    # No math tools should be selected for this query\n",
    "    assert len(result[\"selected_tool_ids\"]) == 0, \"No tools should have been selected for a general knowledge question.\"\n",
    "\n",
    "    final_message = result[\"messages\"][-1]\n",
    "    assert isinstance(final_message, AIMessage)\n",
    "    assert \"Paris\" in final_message.content, \"The agent should provide a direct answer without using a tool.\"\n",
    "\n",
    "\n",
    "def test_multiple_tool_selection_and_execution(agent):\n",
    "    \"\"\"\n",
    "    Tests the agent's ability to select and use multiple tools if implied by the query.\n",
    "    Note: This depends on the agent's underlying logic. A simple query might only\n",
    "    trigger one tool call at a time in many agent designs.\n",
    "    \"\"\"\n",
    "    query = \"What is the sine of 1.57 and the cosine of 0?\"\n",
    "    result = agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    # Verify that both tools were selected\n",
    "    sin_tool_id = _get_tool_id_by_name(\"sin\")\n",
    "    cos_tool_id = _get_tool_id_by_name(\"cos\")\n",
    "    assert sin_tool_id in result[\"selected_tool_ids\"]\n",
    "    assert cos_tool_id in result[\"selected_tool_ids\"]\n",
    "\n",
    "    # Verify the final answer contains both results\n",
    "    final_message = messages[-1]\n",
    "    assert isinstance(final_message, AIMessage)\n",
    "    assert \"1.0\" in final_message.content and \"sine\" in final_message.content.lower()\n",
    "    assert \"1.0\" in final_message.content and \"cosine\" in final_message.content.lower()\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_async_functionality(agent):\n",
    "    \"\"\"Tests the async `ainvoke` method for non-blocking execution.\"\"\"\n",
    "    sqrt_tool_id = _get_tool_id_by_name(\"sqrt\")\n",
    "    query = \"What is the square root of 256?\"\n",
    "\n",
    "    result = await agent.ainvoke({\"messages\": [HumanMessage(content=query)]})\n",
    "\n",
    "    assert sqrt_tool_id in result[\"selected_tool_ids\"]\n",
    "    final_message = result[\"messages\"][-1]\n",
    "    assert isinstance(final_message, AIMessage)\n",
    "    assert \"16\" in final_message.content\n",
    "\n",
    "\n",
    "def test_edge_case_empty_tool_selection(agent):\n",
    "    \"\"\"\n",
    "    Tests the edge case where the LLM correctly determines that no tools are\n",
    "    needed for a conversational query.\n",
    "    \"\"\"\n",
    "    result = agent.invoke({\"messages\": [HumanMessage(content=\"Tell me a short poem about programming.\")]})\n",
    "\n",
    "    assert len(result[\"selected_tool_ids\"]) == 0, \"Tool selection should be empty for a creative request.\"\n",
    "\n",
    "    # Check that a conversational response is given\n",
    "    final_message = result[\"messages\"][-1]\n",
    "    assert isinstance(final_message, AIMessage)\n",
    "    # A simple check to see if it tried to be poetic\n",
    "    assert len(final_message.content.split()) > 5, \"The final message should be a conversational, poetic response.\"\n",
    "\n",
    "\n",
    "# --- Executable Example ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    This block demonstrates how to run the agent directly with a variety of\n",
    "    test cases to showcase its different capabilities.\n",
    "    \n",
    "    You can execute this script from your terminal to see the agent in action.\n",
    "    \n",
    "    Example:\n",
    "        python your_agent_file.py\n",
    "    \"\"\"\n",
    "    print(\"--- Initializing Agent ---\")\n",
    "    # 1. Initialize the Language Model\n",
    "    # Ensure your OPENAI_API_KEY is set as an environment variable\n",
    "    try:\n",
    "        # Using a powerful model is recommended for reliable tool selection\n",
    "        main_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    except ImportError as e:\n",
    "        print(f\"Error: Required packages might be missing. {e}\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        if \"api_key\" in str(e).lower():\n",
    "            print(\"\\n---\")\n",
    "            print(\"ERROR: OpenAI API key not found or invalid.\")\n",
    "            print(\"Please set the OPENAI_API_KEY environment variable.\")\n",
    "            print(\"---\")\n",
    "        else:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Create the agent instance\n",
    "    # The create_agent function returns a compiled graph directly.\n",
    "    agent_executable = create_agent(main_llm, main_llm, tool_registry)\n",
    "    print(\"--- Agent Initialized Successfully ---\\n\")\n",
    "\n",
    "    # 3. Define a list of demonstration cases to run\n",
    "    demonstration_cases = [\n",
    "        {\n",
    "            \"name\": \"SINGLE TOOL USE (FACTORIAL)\",\n",
    "            \"conversation\": [HumanMessage(content=\"What is the factorial of 6?\")],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CONVERSATIONAL HISTORY (SQUARE ROOT)\",\n",
    "            \"conversation\": [\n",
    "                HumanMessage(content=\"My favorite number is 8.\"),\n",
    "                AIMessage(content=\"Got it. Your favorite number is 8. How can I help?\"),\n",
    "                HumanMessage(content=\"What is its square root?\"),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MULTI-TOOL SELECTION (SINE & COSINE)\",\n",
    "            \"conversation\": [\n",
    "                HumanMessage(content=\"What is the sine of 1.57 and the cosine of 0?\")\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "             \"name\": \"TWO-ARGUMENT TOOL (LOG)\",\n",
    "             \"conversation\": [HumanMessage(content=\"What is the log of 1024 with a base of 2?\")]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"NO TOOL SCENARIO (GENERAL KNOWLEDGE)\",\n",
    "            \"conversation\": [\n",
    "                HumanMessage(content=\"What is the capital of France?\")\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EDGE CASE (CREATIVE REQUEST)\",\n",
    "            \"conversation\": [\n",
    "                HumanMessage(content=\"Tell me a short poem about programming.\")\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 4. Iterate through and run each demonstration case\n",
    "    for i, case in enumerate(demonstration_cases):\n",
    "        print(f\"--- Running Demonstration Case {i+1}: {case['name']} ---\")\n",
    "        \n",
    "        initial_messages = case[\"conversation\"]\n",
    "        \n",
    "        print(\"\\nInitial Conversation:\")\n",
    "        for msg in initial_messages:\n",
    "            print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "        # Invoke the agent with the messages\n",
    "        final_result = agent_executable.invoke({\"messages\": initial_messages})\n",
    "\n",
    "        # Print the full final state for review\n",
    "        print(\"\\nFinal Agent State:\")\n",
    "        print(\"  - Selected Tool IDs:\")\n",
    "        print(f\"    {final_result['selected_tool_ids']}\\n\")\n",
    "        \n",
    "        print(\"  - Final Conversation History:\")\n",
    "        for msg in final_result[\"messages\"]:\n",
    "            content = msg.content\n",
    "            # *** FIX STARTS HERE ***\n",
    "            # Safely check if the message object has tool_calls and if they are present.\n",
    "            # This prevents the AttributeError on HumanMessage, ToolMessage, etc.\n",
    "            if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "                # Format the tool calls for cleaner display\n",
    "                calls = [f\"{tc['name']}({tc['args']})\" for tc in msg.tool_calls]\n",
    "                content = f\"Tool Calls: {', '.join(calls)}\"\n",
    "            # *** FIX ENDS HERE ***\n",
    "            \n",
    "            print(f\"    {msg.type.upper()}: {content}\")\n",
    "        print(\"--------------------------------------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f109845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b780b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66991305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
